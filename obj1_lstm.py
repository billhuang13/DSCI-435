# -*- coding: utf-8 -*-
"""obj1_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qVnZYGecjReNTiHLMQxzdXC_bycIWvNy

## Table of Contents

####  **1. Data Preparation and Set Up**

##### 1.1 Import Libraries
##### 1.2 Creating Functions
##### 1.3 Load Data
##### 1.4 Normalizing Variables
##### 1.5 Adding Relevant Variables


<br>

#### **2. Splitting Data**

##### 2.1 Training and Testing Data

<br>

#### **3. Model Training**

##### 3.1 Parameter Tuning
##### 3.2 Model Performance on Training Data

<br>

#### **4. Model Evaluation**

##### 4.1 Predicting on Testing Data
##### 4.2 Evaluate Predictions

#### **1. Data Preparation and Set Up**
##### 1.1 Import Libraries
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error
from scipy import stats

"""##### 1.2 Create Functions"""

def data_normalize(data):
  """
  The function takes in a dataframe like object and returns the normalized data

  Input:
  data: a dataframe like object

  Ouput:
  a dataframe like object that is the normalized version of the original data and the scale of the normalization
  """
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_data = scaler.fit_transform(data)
  return scaled_data, scaler


def data_splitting(df):
  """
  The function takes in a dataframe and splits it as 80% training data and 20% testing data

  Input:
  df, a dataframe

  Output:
  train_data, training data that is 80% of the original data
  test_data, testing data that is 20% of the original data
  """
  train_size = int(len(df) * 0.8)
  train_data, test_data = df[0:train_size], df[train_size:]
  return train_data, test_data



def create_sequences(data, time_steps=1):
  """
  The function takes in a dataframe and a time_steps as a parameter and returns two arrays that represents two lists of sequences
  from the dataset, one represents the features, and the other represents the target variable

  Input:
  data, a dataframe
  time_steps: represents the time length for each sequence, default is set as 1

  Output:
  two arrays, one represents the features, and the other one represents the target variable; each entry in the arrays is of length of
  time_steps
  """
  x, y = [], []
  for i in range(len(data) - time_steps):
      x.append(data.iloc[i:(i + time_steps), :-1])  # everything except for the last column as x
      y.append(data.iloc[i + time_steps, -1])       # last column for y
  return np.array(x), np.array(y)



def lstm(x_train, y_train, epoch, batch_size = 1):
  """
  The function creates an LSTM model from the training data

  Input:
  x_train: the features from training data
  y_train: the target variable from training data
  epoch: a parameter used in the model, represents the number of times the model is trainined on the training data
  batch_size: a parameter used in the model, represents the number of samples for each iteration of training, default is set as 1

  Output: the LSTM model that is created based on the given parameters
  """
  model = Sequential()
  model.add(LSTM(units=128, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
  model.add(LSTM(units=128))
  model.add(Dense(1))

  # Compile the model
  model.compile(optimizer='adam', loss='mean_squared_error')

  # Train the model
  model.fit(x_train, y_train, epochs=epoch, batch_size=batch_size, verbose=0)
  return model

def prediction(model, x):
  """
  The function takes in a model and an x value and returns the prediction based on the x values from the model

  Input:
  model: a regression model, LSTM in this case
  x: the x values that represent the input values for the model

  Output:
  a list object that represents the prediction based on the x values from the model
  """
  return model.predict(x)

def rmse(x, y):
  """
  The function takes in two arrays or lists, and outputs the RMSE between the two arrays or lists

  Input:
  x: an array or list like object
  y: an array of list like object

  Output:
  a float that represents the RMSE between the two objects
  """
  return np.sqrt(mean_squared_error(x, y))


def conf_int(data, confidence_level = 0.95):
  """
  The function takes in a data like object and a confidence level, and returns the lower bounds and upper bounds of the confidence interval
  at the given confidence level

  Input:
  data: a data or list like object
  confidence_level: a float that represents the confidence level of the interval, default set as 0.95 which represents the 95% confidence
                    interval

  Output:
  two list like objects that represent the lower and upper bounds of the confidence interval
  """
  lower = []
  upper = []

  alpha = (1 - confidence_level) / 2
  z = (-1) * stats.norm.ppf(alpha)
  std_dev = data.std()
  margin = z * std_dev

  for i in range(len(data)):
    lower.append(data[i] - margin)
    upper.append(data[i] + margin)

  lower = np.array(lower).flatten()
  upper = np.array(upper).flatten()

  return lower, upper

"""##### 1.3 Load Data"""

if __name__ == '__main__':

  data = pd.read_csv('./data/monthly_data_06_23.csv')

  # A line plot of REMS call counts, which is our target variable

  plt.plot(data['YearMonth'], data['Call Count'])

  plt.xlabel('Date')
  plt.ylabel('Call Counts')
  plt.title('Monthly REMS Call Counts')

  plt.xticks(data['YearMonth'][::10])
  plt.xticks(rotation=45)

  plt.show()

  """##### 1.4 Normalizing Variables"""

  # Dropping the target variable in order to normalize the independent variables and
  # normalize the features using pre-defined function
  features = data.iloc[:,1:].drop('Call Count', axis=1).values
  scaled_features, scaler_features = data_normalize(features)

  # Same process to normalize the target variable
  scaled_target, scaler_target = data_normalize(data['Call Count'].values.reshape(-1, 1))

  """##### 1.5 Adding Relevant Variables"""

  # Adding months to the original data
  rows = data.shape[0]
  data['Month'] = [0] * rows

  for i in range(rows):
    data.loc[i, 'Month'] = data.loc[i, 'YearMonth'].split('-')[1]

  # Converting months to dummy variables
  months = pd.get_dummies(data['Month'])

  data = pd.concat([data, months], axis = 1)
  data.drop(columns = ['Month'], inplace = True)

  # Renaming the month variables
  data['Jan'] = data['01']
  data['Feb'] = data['02']
  data['Mar'] = data['03']
  data['Apr'] = data['04']
  data['Aug'] = data['08']
  data['Sep'] = data['09']
  data['Oct'] = data['10']
  data['Nov'] = data['11']
  data['Dec'] = data['12']
  data.drop(columns=['01','02','03','04','08','09','10','11','12'], inplace=True)

  data

  # New dataframe that has the normalized independent variables and the month variables
  df = pd.concat([pd.DataFrame(scaled_features, columns=['Events', 'Undergraduate', 'Graduate', 'Total']), data.loc[:,['Jan','Feb','Mar','Apr','Aug','Sep','Oct','Nov','Dec']]], axis = 1)
  df['Call Count'] = scaled_target

  # Final dataset that is normalized for the model
  df

  """#### **2. Splitting Data**
  ##### 2.1 Training and Testing Data
  """

  # Splitting data into training and testing data using the pre-defined function
  train_data, test_data = data_splitting(df)

  # Creating training data with sequences
  x_train, y_train = create_sequences(train_data)

  # Creating testing data with sequences
  x_test, y_test = create_sequences(test_data)


  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))
  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))

  # Reversing y_test to original scale
  y_test = scaler_target.inverse_transform(y_test.reshape(-1,1))
  y_test

  """#### **3. Model Training**
  ##### 3.1 Parameter Tuning
  """

  # Create the model using the pre-defined function
  # Parameter tuning is achieved by changing different parameters from the lstm function
  model = lstm(x_train, y_train, epoch = 11)

  """##### 3.2 Model Performance on Training Data"""

  # Predict the values using training data
  # Calculate the training error based on the prediction and the actual value using RMSE
  train_pred = scaler_target.inverse_transform(prediction(model, x_train))
  training_error = rmse(train_pred, y_train)
  training_error

  # Plotting the actual data and the prediction for the training data

  plt.figure(figsize=(9,6))
  plt.plot(data['YearMonth'], data['Call Count'])
  #plt.plot(data['YearMonth'], train_pred, color = 'red')
  plt.plot(train_pred, color = 'orange')


  plt.xlabel('Date')
  plt.ylabel('Call Counts')
  plt.title('Actual and Predicted Call Counts')

  plt.xticks(data['YearMonth'][::10])
  plt.xticks(rotation=45)
  plt.legend(["Actual", "Predicted"], loc=0, frameon=True)

  plt.show()

  """#### **4. Model Evaluation**
  ##### 4.1 Predicting on Testing Data
  """

  # Predicting the testing data using the model
  test_pred = scaler_target.inverse_transform(prediction(model, x_test))
  test_pred

  """##### 4.2 Evaluate Predictions"""

  # Calculating testing error using RMSE
  testing_error = rmse(test_pred, y_test)
  testing_error

  # Plotting the actual data and the predicted data

  plt.figure(figsize=(9,6))
  plt.plot(data['YearMonth'], data['Call Count'])
  plt.plot(data['YearMonth'][112:], test_pred, color = 'red')


  plt.xlabel('Date')
  plt.ylabel('Call Counts')
  plt.title('Actual and Predicted Call Counts')

  plt.xticks(data['YearMonth'][::10])
  plt.xticks(rotation=45)
  plt.legend(["Actual", "Predicted"], loc=0, frameon=True)

  plt.show()

  # Creating two arrays as the lower and upper bounds of the confidence intervals
  lower, upper = conf_int(test_pred)

  # Plotting the actual data and the predicted data with the confidence interval for prediction

  plt.figure(figsize=(9,6))
  plt.plot(data['YearMonth'], data['Call Count'])
  plt.plot(data['YearMonth'][112:], test_pred, color = 'red')

  plt.fill_between(data['YearMonth'][112:], lower, upper, alpha = 0.5, color = 'skyblue')

  plt.xlabel('Date')
  plt.ylabel('Call Counts')
  plt.title('Actual and Predicted Call Counts')

  plt.xticks(data['YearMonth'][::10])
  plt.xticks(rotation=45)
  plt.legend(["Actual", "Predicted"], loc=0, frameon=True)

  plt.show()